{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebabcee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.optimizer as optimizer\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset,random_split\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd72ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "DATASET_DIR = \"./dataset\"\n",
    "RESULTS_DIR = \"./results_mnist_fcnn\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0701326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURES: dict[str, list[int]] = {\n",
    "    \"arch_3h\": [256, 128, 64],\n",
    "    \"arch_4h\": [512, 256, 128, 64],\n",
    "    \"arch_5h\": [512, 256, 128, 64, 32],\n",
    "}\n",
    "CLASSES: set[int] = {\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "}  # NOTE: if we want custom class then use some transformation as it should be from 0 to N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcda7dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "RMSPROP_ALPHA = 0.99  # beta\n",
    "EPS = 1e-8\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "STOP_THRESHOLD = 10e-4  # 0.001\n",
    "MAX_EPOCHS_SAFETY = 20\n",
    "\n",
    "OPTIMIZERS = [\n",
    "    \"SGD_stochastic\",  # batch_size=1\n",
    "    \"BatchGD\",         # batch_size = N_train (full-batch)\n",
    "    \"SGD_momentum\",    # batch_size=1, momentum=0.9\n",
    "    \"SGD_NAG\",         # batch_size=1, nesterov momentum=0.9\n",
    "    \"RMSProp\",         # batch_size=1, alpha=0.99 eps=1e-8\n",
    "    \"Adam\"             # batch_size=1, betas=(0.9,0.999), eps=1e-8\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311d2a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(name: str, params: optimizer.ParamsT, lr: float) -> optimizer.Optimizer:\n",
    "    if name == \"SGD_stochastic\":\n",
    "        return optim.SGD(params, lr=lr)\n",
    "    if name == \"BatchGD\":\n",
    "        return optim.SGD(params, lr=lr)\n",
    "    if name == \"SGD_momentum\":\n",
    "        return optim.SGD(params, lr=lr, momentum=MOMENTUM, nesterov=False)\n",
    "    if name == \"SGD_NAG\":\n",
    "        return optim.SGD(params, lr=lr, momentum=MOMENTUM, nesterov=True)\n",
    "    if name == \"RMSProp\":\n",
    "        return optim.RMSprop(params, lr=lr, alpha=RMSPROP_ALPHA, eps=EPS)\n",
    "    if name == \"Adam\":\n",
    "        return optim.Adam(params, lr=lr, betas=ADAM_BETAS, eps=EPS)\n",
    "    raise ValueError(f\"Unknown optimizer: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934052e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset sizes -> Train: 28588, Test: 7147, classes={0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "def load_filtered_mnist() -> tuple[Subset[torchvision.datasets.MNIST], Subset[torchvision.datasets.MNIST]]:\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)) # Standard normalization for MNIST\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=DATASET_DIR,  # Directory to save the data\n",
    "        train=True,  # Specify training data\n",
    "        download=True,  # Download the data if not present\n",
    "        transform=image_transform,\n",
    "    )\n",
    "    train_dataset = Subset[torchvision.datasets.MNIST](\n",
    "        train_dataset,\n",
    "        [i for i, y in enumerate(train_dataset.targets) if y.item() in CLASSES],\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root=DATASET_DIR,\n",
    "        train=False,  # Specify test data\n",
    "        download=True,\n",
    "        transform=image_transform,\n",
    "    )\n",
    "    test_dataset = Subset[torchvision.datasets.MNIST](\n",
    "        test_dataset,\n",
    "        [i for i, y in enumerate(test_dataset.targets) if y.item() in CLASSES],\n",
    "    )\n",
    "    full_dataset = ConcatDataset[torchvision.datasets.MNIST](\n",
    "        [train_dataset, test_dataset]\n",
    "    )  # as we want 8:2\n",
    "\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    test_size = total_size - train_size\n",
    "\n",
    "    train_set, test_set = random_split(\n",
    "        full_dataset,\n",
    "        [train_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(SEED)  # reproducibility\n",
    "    )\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "train_dataset, test_dataset = load_filtered_mnist()\n",
    "N_train = len(train_dataset)\n",
    "N_test = len(test_dataset)\n",
    "print(f\"Filtered dataset sizes -> Train: {N_train}, Test: {N_test}, classes={CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31f0501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_sizes: list[int],\n",
    "        output_dim: int,\n",
    "        activation: type[nn.Module] = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers: list[nn.Module] = []\n",
    "        prev: int = input_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(activation())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, output_dim))\n",
    "        # NOTE: CrossEntropyLoss expects raw logits, so no softmax here\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c4c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(\n",
    "    model: FCNN, dataloader: DataLoader[torchvision.datasets.MNIST]\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    model.eval()\n",
    "    preds_list: list[np.ndarray] = []\n",
    "    labels_list: list[np.ndarray] = []\n",
    "    with torch.no_grad():\n",
    "        Xb: torch.Tensor\n",
    "        yb: torch.Tensor\n",
    "        logits: torch.Tensor\n",
    "        for Xb, yb in dataloader:\n",
    "            Xb = Xb.reshape(-1, 28*28).to(DEVICE)\n",
    "            logits = model(Xb)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            preds_list.append(preds)\n",
    "            labels_list.append(yb.numpy())\n",
    "    preds_all = np.concatenate(preds_list)\n",
    "    labels_all = np.concatenate(labels_list)\n",
    "    acc = float((preds_all == labels_all).mean())\n",
    "    return preds_all, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900b1cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ARCH arch_3h hidden=[256, 128, 64]\n",
      "  -> Optimizer: SGD_stochastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [06:44<04:29, 33.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=13, avg_train_loss=0.000938, train_acc=1.0000, val_acc=0.9924, time=404.1s\n",
      "  -> Optimizer: BatchGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:07<02:31,  7.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=2, avg_train_loss=1.608717, train_acc=0.2229, val_acc=0.2201, time=8.0s\n",
      "  -> Optimizer: SGD_momentum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [05:18<09:51, 45.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=8, avg_train_loss=0.006911, train_acc=0.9987, val_acc=0.9927, time=318.7s\n",
      "  -> Optimizer: SGD_NAG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [07:24<09:02, 49.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=10, avg_train_loss=0.004535, train_acc=0.9988, val_acc=0.9924, time=444.0s\n",
      "  -> Optimizer: RMSProp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [31:01<00:00, 93.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Safety cap reached (20). Stopping.\n",
      "    done: epochs=20, avg_train_loss=0.402774, train_acc=0.9585, val_acc=0.9544, time=1861.7s\n",
      "  -> Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [54:39<00:00, 163.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Safety cap reached (20). Stopping.\n",
      "    done: epochs=20, avg_train_loss=0.162206, train_acc=0.9835, val_acc=0.9800, time=3279.6s\n",
      "\n",
      "=== ARCH arch_4h hidden=[512, 256, 128, 64]\n",
      "  -> Optimizer: SGD_stochastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [06:17<07:41, 41.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=10, avg_train_loss=0.001463, train_acc=0.9999, val_acc=0.9919, time=377.9s\n",
      "  -> Optimizer: BatchGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:08<02:39,  8.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=2, avg_train_loss=1.614200, train_acc=0.1980, val_acc=0.2074, time=8.4s\n",
      "  -> Optimizer: SGD_momentum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:50<10:50, 65.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=11, avg_train_loss=0.003682, train_acc=0.9990, val_acc=0.9936, time=650.9s\n",
      "  -> Optimizer: SGD_NAG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [07:18<17:04, 73.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=7, avg_train_loss=0.007602, train_acc=0.9976, val_acc=0.9916, time=438.9s\n",
      "  -> Optimizer: RMSProp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [50:49<00:00, 152.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Safety cap reached (20). Stopping.\n",
      "    done: epochs=20, avg_train_loss=2.531674, train_acc=0.5061, val_acc=0.5073, time=3049.8s\n",
      "  -> Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [1:45:03<00:00, 315.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Safety cap reached (20). Stopping.\n",
      "    done: epochs=20, avg_train_loss=0.617513, train_acc=0.9553, val_acc=0.9528, time=6303.5s\n",
      "\n",
      "=== ARCH arch_5h hidden=[512, 256, 128, 64, 32]\n",
      "  -> Optimizer: SGD_stochastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [06:34<08:01, 43.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=10, avg_train_loss=0.000810, train_acc=0.9999, val_acc=0.9919, time=394.3s\n",
      "  -> Optimizer: BatchGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:08<02:35,  8.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=2, avg_train_loss=1.616928, train_acc=0.2145, val_acc=0.2201, time=8.2s\n",
      "  -> Optimizer: SGD_momentum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [06:51<16:01, 68.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=7, avg_train_loss=0.010006, train_acc=0.9990, val_acc=0.9927, time=411.9s\n",
      "  -> Optimizer: SGD_NAG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [09:24<17:29, 80.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    done: epochs=8, avg_train_loss=0.010637, train_acc=0.9971, val_acc=0.9917, time=564.9s\n",
      "  -> Optimizer: RMSProp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [51:18<00:00, 153.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Safety cap reached (20). Stopping.\n",
      "    done: epochs=20, avg_train_loss=1.762907, train_acc=0.5705, val_acc=0.5746, time=3078.2s\n",
      "  -> Optimizer: Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [1:43:44<00:00, 311.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Safety cap reached (20). Stopping.\n",
      "    done: epochs=20, avg_train_loss=0.473146, train_acc=0.9604, val_acc=0.9565, time=6224.4s\n"
     ]
    }
   ],
   "source": [
    "results: list[dict[str, Any]] = []\n",
    "plots_data: dict[str, dict[str, tuple[list[int], list[float]]]] = {}\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for arch_name, hidden_sizes in ARCHITECTURES.items():\n",
    "    print(f\"\\n=== ARCH {arch_name} hidden={hidden_sizes}\")\n",
    "    \n",
    "    base_model = FCNN(784, hidden_sizes, output_dim=len(CLASSES)).to(DEVICE)\n",
    "    plots_data[arch_name] = {}\n",
    "\n",
    "    for opt_name in OPTIMIZERS:\n",
    "        print(f\"  -> Optimizer: {opt_name}\")\n",
    "\n",
    "        # reload model from same initial state\n",
    "        model = FCNN(784, hidden_sizes, output_dim=len(CLASSES)).to(DEVICE)\n",
    "        model.load_state_dict(base_model.state_dict())\n",
    "\n",
    "        # set optimizer and batch size policy\n",
    "        if opt_name == \"BatchGD\":\n",
    "            batch_size = N_train  # full-batch\n",
    "            shuffle = False\n",
    "        else:\n",
    "            batch_size = 1\n",
    "            shuffle = True\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        test_loader  = DataLoader(test_dataset,  batch_size=128, shuffle=False)  # for evaluation; 128 is fine\n",
    "\n",
    "        curr_optimizer = get_optimizer(opt_name, model.parameters(), LR)\n",
    "\n",
    "        epoch = 0\n",
    "        prev_avg_loss = None\n",
    "        avg_loss_history: list[float] = []\n",
    "        epoch_history: list[int] = []\n",
    "        converged = False\n",
    "        start_time = time.time()\n",
    "        avg_loss: float | None = None\n",
    "\n",
    "        # Run epochs until stopping criterion triggered (or safety cap)\n",
    "        for _ in tqdm(range(MAX_EPOCHS_SAFETY)):\n",
    "            epoch += 1\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            total_samples = 0\n",
    "            # iterate over batches\n",
    "            Xb: torch.Tensor\n",
    "            yb: torch.Tensor\n",
    "            for Xb, yb in train_loader:\n",
    "                Xb = Xb.reshape(-1, 28*28).to(DEVICE)\n",
    "                yb = yb.to(DEVICE)\n",
    "\n",
    "                curr_optimizer.zero_grad()\n",
    "                logits = model(Xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "                loss.backward()\n",
    "                curr_optimizer.step()\n",
    "\n",
    "                b = Xb.shape[0]\n",
    "                running_loss += float(loss.item()) * b\n",
    "                total_samples += b\n",
    "\n",
    "            avg_loss = running_loss / (total_samples + 1e-12)\n",
    "            avg_loss_history.append(avg_loss)\n",
    "            epoch_history.append(epoch)\n",
    "\n",
    "            # compute train/val accuracy for monitoring\n",
    "            _, train_acc = eval_model(model, DataLoader(train_dataset, batch_size=256, shuffle=False))\n",
    "            _, val_acc   = eval_model(model, test_loader)\n",
    "\n",
    "            # stopping based on absolute difference in successive epoch average training error\n",
    "            if prev_avg_loss is not None:\n",
    "                if abs(avg_loss - prev_avg_loss) < STOP_THRESHOLD:\n",
    "                    converged = True\n",
    "\n",
    "            prev_avg_loss = avg_loss\n",
    "\n",
    "            if converged:\n",
    "                break\n",
    "        # safety cap: not used as stopping criterion, only to prevent infinite loops\n",
    "        else:\n",
    "            print(f\"    Safety cap reached ({MAX_EPOCHS_SAFETY}). Stopping.\")\n",
    "\n",
    "        assert avg_loss is not None, \":-(\"\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # final evaluation & confusion matrices\n",
    "        preds_train, train_acc_final = eval_model(model, DataLoader(train_dataset, batch_size=256, shuffle=False))\n",
    "        preds_test, test_acc_final   = eval_model(model, test_loader)\n",
    "\n",
    "        # labels recovered from dataset subsets\n",
    "        train_labels = np.concatenate([yb.numpy() for _, yb in DataLoader(train_dataset, batch_size=256, shuffle=False)])\n",
    "        test_labels  = np.concatenate([yb.numpy() for _, yb in DataLoader(test_dataset,  batch_size=128, shuffle=False)])\n",
    "\n",
    "        cm_train = confusion_matrix(train_labels, preds_train)\n",
    "        cm_test  = confusion_matrix(test_labels, preds_test)\n",
    "\n",
    "        print(f\"    done: epochs={epoch}, avg_train_loss={avg_loss:.6f}, train_acc={train_acc_final:.4f}, val_acc={test_acc_final:.4f}, time={elapsed:.1f}s\")\n",
    "\n",
    "        results.append({\n",
    "            \"architecture\": arch_name,\n",
    "            \"hidden_sizes\": hidden_sizes,\n",
    "            \"optimizer\": opt_name,\n",
    "            \"lr\": LR,\n",
    "            \"epochs_to_converge\": epoch,\n",
    "            \"final_avg_train_loss\": avg_loss,\n",
    "            \"train_accuracy\": train_acc_final,\n",
    "            \"val_accuracy\": test_acc_final,\n",
    "            \"time_s\": elapsed,\n",
    "            \"cm_train\": cm_train,\n",
    "            \"cm_test\": cm_test\n",
    "        })\n",
    "\n",
    "        plots_data[arch_name][opt_name] = (epoch_history, avg_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b4f51f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary CSV: ./results_mnist_fcnn/summary_results.csv\n"
     ]
    }
   ],
   "source": [
    "df_rows: list[dict[str, Any]] = []\n",
    "for r in results:\n",
    "    df_rows.append({\n",
    "        \"architecture\": r[\"architecture\"],\n",
    "        \"hidden_sizes\": \",\".join(map(str, r[\"hidden_sizes\"])),\n",
    "        \"optimizer\": r[\"optimizer\"],\n",
    "        \"lr\": r[\"lr\"],\n",
    "        \"epochs_to_converge\": r[\"epochs_to_converge\"],\n",
    "        \"final_avg_train_loss\": r[\"final_avg_train_loss\"],\n",
    "        \"train_accuracy\": r[\"train_accuracy\"],\n",
    "        \"val_accuracy\": r[\"val_accuracy\"],\n",
    "        \"time_s\": r[\"time_s\"]\n",
    "    })\n",
    "df = pd.DataFrame(df_rows)\n",
    "csv_path = os.path.join(RESULTS_DIR, \"summary_results.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(\"Saved summary CSV:\", csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ffa26d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot: ./results_mnist_fcnn/arch_3h_loss_curves.png\n",
      "Saved plot: ./results_mnist_fcnn/arch_4h_loss_curves.png\n",
      "Saved plot: ./results_mnist_fcnn/arch_5h_loss_curves.png\n"
     ]
    }
   ],
   "source": [
    "# plot curves per architecture\n",
    "for arch_name, opt_data in plots_data.items():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for opt_name, (epochs, losses) in opt_data.items():\n",
    "        plt.plot(epochs, losses, label=opt_name)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average training loss\")\n",
    "    plt.title(f\"Avg training loss vs epoch ({arch_name})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    pth = os.path.join(RESULTS_DIR, f\"{arch_name}_loss_curves.png\")\n",
    "    plt.savefig(pth)\n",
    "    plt.close()\n",
    "    print(\"Saved plot:\", pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ff2acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best architecture by validation accuracy and save confusion matrices\n",
    "best = max(results, key=lambda x: x[\"val_accuracy\"])\n",
    "best_report = {\n",
    "    \"best_architecture\": best[\"architecture\"],\n",
    "    \"hidden_sizes\": best[\"hidden_sizes\"],\n",
    "    \"best_optimizer\": best[\"optimizer\"],\n",
    "    \"best_val_accuracy\": best[\"val_accuracy\"],\n",
    "    \"train_accuracy\": best[\"train_accuracy\"],\n",
    "    \"cm_train\": best[\"cm_train\"].tolist(),\n",
    "    \"cm_test\": best[\"cm_test\"].tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e2f1fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model report (JSON).\n",
      "All done. Results in: ./results_mnist_fcnn\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(os.path.join(RESULTS_DIR, \"best_model_report.json\"), \"w\") as f:\n",
    "    json.dump(best_report, f, indent=2)\n",
    "print(\"Saved best model report (JSON).\")\n",
    "\n",
    "print(\"All done. Results in:\", RESULTS_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
